{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\a1248\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentment total samples 5000\n",
      "negative sentment total samples 5000\n",
      "Let's take a look at first 2 sample tweets:\n",
      "\n",
      "Example of Positive tweets:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "\n",
      "Example of Negative tweets:\n",
      "hopeless for tmr :(\n",
      "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "\n",
      "Tweets may have URLs,numbers,and special characters.Hence,we need to process the text.\n"
     ]
    }
   ],
   "source": [
    "# regular expression opertation\n",
    "import re\n",
    "# string operation\n",
    "import numpy\n",
    "import string\n",
    "# shuffle the list\n",
    "from random import shuffle\n",
    "# Linear algebra\n",
    "import numpy as np\n",
    "# data processing \n",
    "import pandas as pd\n",
    "# NLTK library & download twitter dataset\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "# module for stop words that come with NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# module for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# module for tokenizing strings\n",
    "from nltk.tokenize import TweetTokenizer as tt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#smart progressor meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"twitter_samples\")\n",
    "# read the positive and negative tweets\n",
    "pos_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "neg_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "print(f\"positive sentment total samples {len(pos_tweets)}\\nnegative sentment total samples {len(neg_tweets)}\")\n",
    "\n",
    "\n",
    "\n",
    "no_of_tweets = 2\n",
    "print(f\"Let's take a look at first {no_of_tweets} sample tweets:\\n\")\n",
    "print(\"Example of Positive tweets:\")\n",
    "print(\"\\n\".join(pos_tweets[:no_of_tweets]))\n",
    "print(\"\\nExample of Negative tweets:\")\n",
    "print(\"\\n\".join(neg_tweets[:no_of_tweets]))\n",
    "print(\"\\nTweets may have URLs,numbers,and special characters.Hence,we need to process the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 4366.21it/s]\n",
      "5000it [00:01, 4553.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!']\n",
      "=============================================\n",
      "[['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'], ['hey', 'jame', 'odd', ':/', 'pleas', 'call', 'contact', 'centr', 'abl', 'assist', ':)', 'mani', 'thank']]\n",
      "=============================================\n",
      "train_X_tweet (8000,),test_X_tweet(2000,),train_Y(8000,),test_Y(2000,)\n",
      "=============================================\n",
      "train_X (8000,),test_X_tweet(2000,)\n",
      "=============================================\n",
      "[[1.000e+00 6.200e+01 3.500e+01]\n",
      " [1.000e+00 6.330e+02 0.000e+00]\n",
      " [1.000e+00 3.582e+03 2.000e+00]\n",
      " [1.000e+00 3.582e+03 2.000e+00]\n",
      " [1.000e+00 3.582e+03 2.000e+00]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Twitter_Preprocess' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-02924680bf1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;31m# Apply the gradient descent of Logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;31m# 0.1 as added L2 regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The cost after traning is {J:.4f}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-02924680bf1d>\u001b[0m in \u001b[0;36mgradientDescent\u001b[1;34m(train_X, train_Y, theta, J, y_pred, learning_rate, iterations)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Twitter_Preprocess' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "#Helper class for doing preprocrssing\n",
    "class Twitter_Preprocess():\n",
    "    def __init__(self):\n",
    "        # instantiate tokenizer class\n",
    "        self.tokenizer = tt(preserve_case=False,strip_handles=True,reduce_len=True)\n",
    "        # get the EN stopwords\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "        # get the EN punctuation\n",
    "        self.punctuation = string.punctuation\n",
    "        # Instantiate stemmer object\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "    def __remove_unwanted_characters__(self,tweet):\n",
    "        \n",
    "        # remove retweet style text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+','',tweet)\n",
    "        \n",
    "        # remove hyperlink\n",
    "        tweet = re.sub(r'http?:\\/\\/.*[\\r\\n]*','',tweet)\n",
    "        \n",
    "        # remove hashtag\n",
    "        tweet = re.sub(r'#','',tweet)\n",
    "        \n",
    "        # remove email address\n",
    "        tweet = re.sub(r'\\S+@\\S+','',tweet)\n",
    "        \n",
    "        # remove numbers\n",
    "        tweet = re.sub(r'\\d+','',tweet)\n",
    "        \n",
    "        # return removed text\n",
    "        return tweet\n",
    "    \n",
    "    def __tokenize_tweet__(self,tweet):\n",
    "        return self.tokenizer.tokenize(tweet)\n",
    "    \n",
    "    def __remove_stopwords__(self,tweet_tokens):\n",
    "        \n",
    "        tweets_clean = []\n",
    "        \n",
    "        for word in tweet_tokens:\n",
    "            if (word not in self.stopwords and word not in self.punctuation):\n",
    "                tweets_clean.append(word)\n",
    "        return tweets_clean\n",
    "    \n",
    "    def __text_stemming__(self,tweet_tokens):\n",
    "        # store the stemmed word\n",
    "        tweets_stem = []\n",
    "        \n",
    "        for word in tweet_tokens:\n",
    "            # stemming word\n",
    "            stem_word = self.stemmer.stem(word)\n",
    "            tweets_stem.append(stem_word)\n",
    "        return tweets_stem # 注意 tab !! \n",
    "        \n",
    "    def preprocess(self,tweets):\n",
    "        tweets_processed = []\n",
    "        for _,tweet in tqdm(enumerate(tweets)):\n",
    "            # apply removing unwanted characters and remove style of retweet,URL\n",
    "            tweet = self.__remove_unwanted_characters__(tweet)\n",
    "            # apply nltk tokenizer\n",
    "            tweet_tokens = self.__tokenize_tweet__(tweet)\n",
    "            # apply stop word removal\n",
    "            tweets_clean = self.__remove_stopwords__(tweet_tokens)\n",
    "            # apply stemmer\n",
    "            tweet_stems = self.__text_stemming__(tweets_clean)\n",
    "            tweets_processed.extend([tweet_stems])\n",
    "        return tweets_processed\n",
    "  \n",
    "    \n",
    "    def build_bow_dict(self,tweets,labels):\n",
    "        freq = {}\n",
    "        for tweet,label in list(zip(tweets,labels)):\n",
    "            for word in tweet : \n",
    "                freq[(word,label)] = freq.get((word,label),0) + 1\n",
    "                \n",
    "        return freq\n",
    "    \n",
    "    def extract_features(self,processed_tweet,bow_word_freq):\n",
    "        # features array\n",
    "        features = np.zeros((1,3))\n",
    "        # bias term added in the 0th index\n",
    "        features[0,0] = 1\n",
    "        \n",
    "        for word in processed_tweet:\n",
    "            # get the positive freq of the word\n",
    "            features[0,1] = bow_word_freq.get((word,1),0)\n",
    "            # get the negative freq of the word\n",
    "            features[0,2] = bow_word_freq.get((word,0),0)\n",
    "            \n",
    "        return features\n",
    "           \n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        h = 1 / (1+np.exp(-z))\n",
    "        return h\n",
    "    \n",
    "    def predict_tweet(train_X,theta):\n",
    "        \n",
    "        y_pred = sigmoid(np.dot(train_X,theta))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def gradientDescent(train_X,train_Y,theta,J,y_pred,learning_rate=0.01,iterations=1000):\n",
    "        \n",
    "        m = len(train_Y)\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            error = y_pred - train_Y\n",
    "            gradients = np.dot(train_X.T,error)\n",
    "            theta = theta - (1/m)*learning_rate*gradients\n",
    "     \n",
    "        return theta\n",
    "    \n",
    "# initialize the text preprocessor class object\n",
    "tp = Twitter_Preprocess()\n",
    "# process tje positive and negative tweets\n",
    "processed_pos_tweets = tp.preprocess(pos_tweets)\n",
    "processed_neg_tweets = tp.preprocess(neg_tweets)\n",
    "print(pos_tweets[:no_of_tweets])\n",
    "print(\"=============================================\")\n",
    "print(processed_pos_tweets[:no_of_tweets])\n",
    "print(\"=============================================\")\n",
    "\n",
    "labels = [1 for i in range(len(processed_pos_tweets))]\n",
    "labels.extend([0 for i in range(len(processed_neg_tweets))])\n",
    "    \n",
    "twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n",
    "bow_word_freq = tp.build_bow_dict(twitter_processed_corpus, labels)\n",
    "\n",
    "# shuffle the positive & negative tweets\n",
    "shuffle(processed_pos_tweets)\n",
    "shuffle(processed_neg_tweets)\n",
    "# 建立標籤\n",
    "positive_tweet_label = [1 for i in processed_pos_tweets]\n",
    "negative_tweet_label = [0 for i in processed_neg_tweets]\n",
    "# 建立資料架構\n",
    "tweet_df = pd.DataFrame(list(zip( twitter_processed_corpus,positive_tweet_label+negative_tweet_label)),columns = [\"processed_tweet\",\"label\"])\n",
    "    \n",
    "# train & test split\n",
    "train_X_tweet,test_X_tweet,train_Y,test_Y = train_test_split(tweet_df[\"processed_tweet\"],tweet_df[\"label\"],test_size = 0.2,stratify=tweet_df[\"label\"])\n",
    "print(f\"train_X_tweet {train_X_tweet.shape},test_X_tweet{test_X_tweet.shape},train_Y{train_Y.shape},test_Y{test_Y.shape}\")\n",
    "print(\"=============================================\")\n",
    "    \n",
    "# train X feature dimension\n",
    "train_X = np.zeros((len(train_X_tweet),3))\n",
    "    \n",
    "for index,tweet in enumerate(train_X_tweet):\n",
    "    train_X[index, :] = tp.extract_features(tweet,bow_word_freq)\n",
    "        \n",
    "# test X feature dimension\n",
    "test_X = np.zeros((len(test_X_tweet),3))\n",
    "    \n",
    "for index,tweet in enumerate(test_X_tweet):\n",
    "        test_X[index, :] = tp.extract_features(tweet,bow_word_freq)\n",
    "print(f\"train_X {train_X_tweet.shape},test_X_tweet{test_X_tweet.shape}\")\n",
    "print(\"=============================================\")\n",
    "print(train_X[0:5])\n",
    "\n",
    "# set the seed in numpy\n",
    "np.random.seed(1)\n",
    "# Apply the gradient descent of Logistic regression\n",
    "# 0.1 as added L2 regularization\n",
    "J,theta = tp.gradientDescent(train_X,np.array(train_Y).reshape(-1,1), np.zeros((3,1)), 1e-7)\n",
    "    \n",
    "print(f\"The cost after traning is {J:.4f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t,4) for t in np.squeeze(theta)]}\")\n",
    "\n",
    "predicted_probs = predict_tweet(test_X,theta)\n",
    "predicted_labels = np.where(predicted_probs, 0.5, 1, 0)\n",
    "print(f\"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)])/len(test_Y)*100:.2f}\")\n",
    "    \n",
    "clf = LogisticRegression(random_state=42,penalty=\"12\")\n",
    "clf.fit(train_X,np.array(train_Y).reshape(-1,1))\n",
    "y_pred = clf.predict(test_X)\n",
    "y_pred_probs = clf.predict(test_X)\n",
    "print(f\"SKlearn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "==========================================================================================================\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "s = '#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'\n",
    "re.sub(r'\\d+', '', s) \n",
    "re.sub(r'#','',s)\n",
    "re.sub(r'@','',s)\n",
    "print(s)\n",
    "print(\"==========================================================================================================\")\n",
    "print(type(train_X.T))\n",
    "print(type(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] \n",
      "\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]] \n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "x=np.array([1,2,3])\n",
    "train_X = np.zeros((len(x),3))\n",
    "print(x,\"\\n\")\n",
    "print(train_X,\"\\n\")\n",
    "y=np.array([1,1,1])\n",
    "s=np.dot(x,y)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
